{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "814123ef-9ea3-4081-88e4-3db35e277e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "546d826b-7b18-4734-80b2-63c653a57aef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: azuki_default.png\n",
      " 1: ceo_default.png\n",
      " 2: check1.png\n",
      " 3: check2.png\n",
      " 4: danda_default.png\n",
      " 5: detective.png\n",
      " 6: detective_02.png\n",
      " 7: idPhoto.png\n",
      " 8: mio_happy.png\n",
      " 9: mio_shock.png\n",
      "10: mio_silence.png\n",
      "11: mio_u.png\n",
      "12: nanko_default.png\n",
      "13: ookawa_angry.png\n",
      "14: ookawa_angry2.png\n",
      "15: ookawa_default.png\n",
      "16: ookawa_high.png\n",
      "17: ookawa_regret.png\n",
      "18: ookawa_smile.png\n",
      "19: ookawa_surprised.png\n",
      "20: pharmacist.png\n",
      "21: saki.png\n",
      "22: saki_glasses.png\n",
      "23: sandy.png\n",
      "24: takebe_default.png\n",
      "25: test01.png\n",
      "26: test01_02.png\n",
      "27: test01_03.png\n",
      "28: test01_04.png\n",
      "29: test02.png\n",
      "30: woman_default.png\n",
      "31: yotaka_angry.png\n",
      "32: yotaka_angry2.png\n",
      "33: yotaka_bald.png\n",
      "34: yotaka_bushy.png\n",
      "35: yotaka_default.png\n",
      "36: yotaka_gj.png\n",
      "37: yotaka_smile.png\n",
      "38: yotaka_smile2.png\n",
      "\n",
      "total: 39\n"
     ]
    }
   ],
   "source": [
    "img_dir = 'imgs_align'\n",
    "num_files = 0\n",
    "for i,file in enumerate(os.listdir(img_dir)):\n",
    "    print(f'{i:2}: {file}')\n",
    "    num_files += 1\n",
    "print(f'\\ntotal: {num_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1fe7cc-6461-4877-84e9-85dbc07b61bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2d7923c-5efd-49cf-8328-558e95469a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos(a, b):\n",
    "    a = a.view(-1)\n",
    "    b = b.view(-1)\n",
    "    a = F.normalize(a, dim=0)\n",
    "    b = F.normalize(b, dim=0)\n",
    "    return (a * b).sum()\n",
    "\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c46fbd3a-f301-4d22-a648-b5be829cabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_face1 = np.zeros((256,256,3)).astype(np.uint64)\n",
    "avg_face2 = torch.zeros(3,256,256)\n",
    "for i,file in enumerate(os.listdir(img_dir)):\n",
    "    img1 = cv2.imread(f'{img_dir}/{file}')\n",
    "    avg_face1 += img1\n",
    "    img2 = Image.open(f'{img_dir}/{file}')\n",
    "    img2 = transform(img2)\n",
    "    avg_face2 += img2\n",
    "\n",
    "avg_face1 = avg_face1 // num_files\n",
    "avg_face1 = avg_face1.astype(np.uint8)\n",
    "avg_face2 = avg_face2 / num_files\n",
    "avg_face2 = avg_face2+1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e201faac-67ae-4b90-9880-ec895d2c0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_face3 = 0\n",
    "for i,file in enumerate(os.listdir(img_dir)):\n",
    "    img = Image.open(f'{img_dir}/{file}')\n",
    "    img = transform(img)\n",
    "    if i == 0:\n",
    "        avg_face3 = img.flatten(0, 2)\n",
    "    else:\n",
    "        img = img.flatten(0, 2)\n",
    "        theta = torch.arccos(cos(avg_face3, img))\n",
    "        avg_face3 = (torch.sin((1 - alpha) * theta) * avg_face3 + torch.sin(alpha * theta) * img) / torch.sin(theta)\n",
    "avg_face3 = avg_face3.view(-1, *(3, 256, 256))\n",
    "avg_face3 = avg_face3+1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d22baa2f-7c73-49df-8f3b-cdf54a846afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_dir = 'imgs_test/imgs_avg/'\n",
    "if not os.path.exists(dst_dir): os.makedirs(dst_dir)\n",
    "cv2.imwrite(f'{dst_dir}avg_face1.png', avg_face1)\n",
    "save_image(avg_face2, f'{dst_dir}avg_face2.png', format='PNG')\n",
    "save_image(avg_face3, f'{dst_dir}avg_face3.png', format='PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895c980-8d25-4864-a09e-10974646560e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
